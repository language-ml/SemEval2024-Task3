{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54c90cc-5f53-459f-b44a-77f025c89b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de71db1-95de-4ed1-8861-77906f7d0bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance \n",
    "\n",
    "def find_best_subsequence(answer, context):\n",
    "    context = context.split()\n",
    "    \n",
    "    min_dist = distance(context, answer)\n",
    "    best_ij = 0, len(context)\n",
    "    \n",
    "    for i in range(0, len(context)):\n",
    "        for j in range(i + 1, len(context)):\n",
    "            subsequence = \" \".join(context[i:j])\n",
    "            dist = distance(subsequence, answer)\n",
    "            if min_dist > dist:\n",
    "                min_dist = dist\n",
    "                best_ij = i, j\n",
    "    \n",
    "    return best_ij\n",
    "\n",
    "find_best_subsequence(\"BBB CCC\", \"AAA BBB CCC DDD EEEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde13699-bb0a-4c21-9e06-38e7db97a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import peft\n",
    "import string\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14793b-518e-4890-8533-4f4c7820a66d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e2eeed-5d03-44c4-99ce-8418332da9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tae898/emoberta-large\")\n",
    "model_l0 = AutoModelForSequenceClassification.from_pretrained(\"tae898/emoberta-large\")\n",
    "\n",
    "model_l0.load_state_dict(torch.load(\"models/model_l0_v07.pth\", map_location=\"cpu\"))\n",
    "_ = model_l0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0f3bef-15d8-42be-ab43-f5dd203aac9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355366919, 1053440)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.normalizer = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.normalizer(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x, weights = self.self_attn(x, x, x, attn_mask=mask)    \n",
    "        return x, weights\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, logit_dim, model_dim, n_layers, n_heads, ff_dim, max_len, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "  \n",
    "        self.compressor = nn.Linear(embed_dim + logit_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_len, model_dim).permute(1, 0, 2)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(model_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "\n",
    "    def get_positional_encoding(self, max_len, d_model):\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(np.log(10000.0) / d_model))\n",
    "        pos_enc = torch.zeros((max_len, 1, d_model))\n",
    "        pos_enc[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        \n",
    "        x = torch.concatenate((x, y), dim=-1)\n",
    "        x = self.compressor(x)   \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x + self.positional_encoding[:, :x.shape[1], :].to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, weights = layer(x, mask)\n",
    "           \n",
    "        return x, weights\n",
    "    \n",
    "model_l1 = TransformerEncoder(\n",
    "    embed_dim=1024, model_dim=256, logit_dim=7, n_layers=1, n_heads=1, ff_dim=1024, max_len=40)\n",
    "\n",
    "model_l1.load_state_dict(torch.load(\"models/model_l1_v09.pth\", map_location=\"cpu\"))\n",
    "model_l1.eval()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model_l0), count_parameters(model_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7027d4e-f3e4-4e64-96cb-89f6e289abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "checkpoint = \"models/qa/checkpoint-10500\"\n",
    "model_name = \"deepset/deberta-v3-base-squad2\"\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_model = PeftModel.from_pretrained(qa_model, checkpoint)\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005bd992-0664-4731-9255-72af4db38007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def generate_emotion_causes(model_l0, model_l1, tokenizer, all_texts, all_speaker):\n",
    "    \n",
    "    model_l0.to(device)\n",
    "    model_l1.to(device)\n",
    "    \n",
    "    texts = [f\"speaker: {s} dialog: {d}\" for s, d in zip(all_speaker, all_texts)]\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model_l0(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        embeds = output.hidden_states[-1][:, 0, :]\n",
    "        logits = F.softmax(output.logits, dim=-1)\n",
    "\n",
    "    embeds, logits = embeds.unsqueeze(0), logits.unsqueeze(0)\n",
    "    \n",
    "    weights = model_l1(embeds, logits)[1].squeeze(0)\n",
    "    \n",
    "    cause_org, cause_dst = torch.where(weights >= 0.2)\n",
    "    cause_org = cause_org.tolist()\n",
    "    cause_dst = cause_dst.tolist()\n",
    "    \n",
    "    cpairs = list(zip(cause_org, cause_dst))\n",
    "    logits = logits.squeeze(0)\n",
    "    \n",
    "    emotions = logits.max(dim=-1)[1]\n",
    "    emotions = [model_l0.config.id2label[x] for x in emotions.tolist()]\n",
    "        \n",
    "    return emotions, cpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449013e-0995-4d54-ad5c-b53391ea594a",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3a2447-858a-4665-b602-ebc878799030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data):\n",
    "    final_results = []\n",
    "    for conversation in tqdm(data):\n",
    "\n",
    "        sample = conversation[\"conversation\"]\n",
    "        all_texts = [d[\"text\"] for d in sample]\n",
    "        all_speaker = [d[\"speaker\"] for d in sample]\n",
    "\n",
    "        emotions, cpairs = generate_emotion_causes(model_l0, model_l1, tokenizer, all_texts, all_speaker)\n",
    "\n",
    "        for i, e in enumerate(emotions):\n",
    "            if e == \"neutral\":\n",
    "                cpairs = [x for x in cpairs if x[0] != i]\n",
    "            else:\n",
    "                if any(x[0] == i for x in cpairs):\n",
    "                    continue\n",
    "                cpairs.append((i, i - 1))\n",
    "\n",
    "        if len(cpairs) > 0:\n",
    "            prompt = \"Which part of the text ’{}’ is the reason for ’ {} ’'s feeling of ’ {} ’ when ’ {} ’ is said?\"\n",
    "\n",
    "            all_questions = []\n",
    "            all_contexts = []\n",
    "\n",
    "            for org, dst in cpairs:\n",
    "                d1 = sample[org]\n",
    "                d2 = sample[dst]\n",
    "\n",
    "                all_questions.append(prompt.format(d2[\"text\"], d1[\"speaker\"], emotions[org], d1[\"text\"]))\n",
    "                all_contexts.append(d2[\"text\"])\n",
    "\n",
    "            inputs = tokenizer_qa(all_questions, all_contexts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = qa_model(**inputs)\n",
    "\n",
    "            answer_start_index, answer_end_index = \\\n",
    "                outputs.start_logits.argmax(dim=-1).tolist(), outputs.end_logits.argmax(dim=-1).tolist()\n",
    "\n",
    "            answers = []\n",
    "\n",
    "            for i, (start, end, context) in enumerate(zip(answer_start_index, answer_end_index, all_contexts)):\n",
    "                \n",
    "                context = context[::-1]\n",
    "                while context[0] in string.punctuation:\n",
    "                    context = context[1:]\n",
    "\n",
    "                context = context[::-1]\n",
    "                while context[0] in string.punctuation:\n",
    "                    context = context[1:]\n",
    "\n",
    "                try:\n",
    "                    answer = tokenizer_qa.decode(inputs.input_ids[i, start:end])\n",
    "                    answer = answer[::-1]\n",
    "                    while answer[0] in string.punctuation:\n",
    "                        answer = answer[1:]\n",
    "\n",
    "                    answer = answer[::-1]\n",
    "                    while answer[0] in string.punctuation:\n",
    "                        answer = answer[1:]\n",
    "\n",
    "                    answer.strip()\n",
    "                    answers.append(find_best_subsequence(answer, context))\n",
    "\n",
    "                except IndexError:\n",
    "                    answer = context\n",
    "                    answer = answer[::-1]\n",
    "                    while answer[0] in string.punctuation:\n",
    "                        answer = answer[1:]\n",
    "\n",
    "                    answer = answer[::-1]\n",
    "                    while answer[0] in string.punctuation:\n",
    "                        answer = answer[1:]\n",
    "\n",
    "                    answer.strip()\n",
    "                    answers.append((0, len(answer.split())))\n",
    "\n",
    "            results = []\n",
    "            for i, (org, dst) in enumerate(cpairs):\n",
    "                d1 = sample[org]\n",
    "                d2 = sample[dst]\n",
    "\n",
    "                results.append([\"{}_{}\".format(d1[\"utterance_ID\"], emotions[org]), \"{}_{}_{}\".format(d2[\"utterance_ID\"], answers[i][0], answers[i][1])])\n",
    "\n",
    "            conversation = copy.deepcopy(conversation)\n",
    "            conversation.update({\"emotion-cause_pairs\": results})\n",
    "        else:\n",
    "            conversation = copy.deepcopy(conversation)\n",
    "            conversation.update({\"emotion-cause_pairs\": []})\n",
    "\n",
    "        final_results.append(conversation)\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15e243c-72fe-47e0-a1f7-c4d3b764c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 665/665 [03:02<00:00,  3.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Subtask_1_test.json\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "final_results = func(data)\n",
    "with open(\"Subtask_1_pred.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10debb6e-c295-4583-93f3-c3f2ec759ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 665/665 [02:55<00:00,  3.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Subtask_2_test.json\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "final_results = func(data)\n",
    "with open(\"Subtask_2_pred.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc35a59-de23-4ffc-a25a-c7375b19f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Subtask_2_pred.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "json_formatted_str = json.dumps(data, indent=4)\n",
    "for i in range(len(data)):\n",
    "  ecps = data[i][\"emotion-cause_pairs\"]\n",
    "  for j in range(len(ecps)):\n",
    "    ecp = ecps[j]\n",
    "    l = ecp[1][:ecp[1].find(\"_\")]\n",
    "    data[i][\"emotion-cause_pairs\"][j][1] = l\n",
    "\n",
    "with open('Subtask_2_pred.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
